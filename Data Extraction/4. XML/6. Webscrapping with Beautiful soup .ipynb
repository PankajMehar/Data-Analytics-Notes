{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. BeautifulSoup gives us the ability to parse the HTML document tree. Its similar to parsing the element tree in the XML document.\n",
    "\n",
    "Beautiful Soup is an HTML/XML parser for Python that can turn even invalid markup into a parse tree. It provides simple, idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n",
    "\n",
    "Beautiful Soup 4 is faster, has more features, and works with third-party parsers\n",
    "like lxml and html5lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This [webpage](https://www.transtats.bts.gov/Data_Elements.aspx?Data=2) is used for web-scrapping with the help of beautifulsoup. Choose the following in the above page :\n",
    "\n",
    "__Airport :__ Bostan MA Logan International\n",
    "\n",
    "__ Carrier :__ Virgin America\n",
    "\n",
    "## Data Wrangling Procedure\n",
    "* Build list of carrier values.\n",
    "* Build list of airport values.\n",
    "* Make HTTP requests to download all data.\n",
    "* Then parse the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    " # Convert the html content into a beautiful soup object\n",
    " # 'lxml' used to choose html parser (and to avoid warning)   \n",
    "soup = BeautifulSoup(open(\"virgin_and_logan_airport.html\"),'lxml')   \n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>\n",
       "\tData Elements\n",
       "</title>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\tData Elements\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'head'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>BUREAU OF TRANSPORTATION STATISTICS</p>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<option selected=\"selected\" value=\"All\">All U.S. and Foreign Carriers</option>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For beautiful soup it is find_all() but for xml it is findall()\n",
    "\n",
    "\n",
    "\n",
    "## __soup.find_all() :__ returns a list with all parent/children element with a known tag.\n",
    "\n",
    "find_all() will find all of its descendants instead of just the first one.\n",
    "\n",
    "_Check what if had another html tag option in the file???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "option_list = soup.find_all('option') \n",
    "print(type(option_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __soup.find() :__ Finds the first element/child with a particular tag along with its children\n",
    "\n",
    "\n",
    "The only difference is that find_all() returns a list containing the single result, and find() just returns the result.\n",
    "\n",
    "If find_all() can’t find anything, it returns an empty list. If find() can’t find anything, it returns None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<select class=\"slcBox\" id=\"CarrierList\" name=\"CarrierList\" style=\"width:450px;\">\n",
       "<option selected=\"selected\" value=\"All\">All U.S. and Foreign Carriers</option>\n",
       "<option value=\"AllUS\">All U.S. Carriers</option>\n",
       "<option value=\"AllForeign\">All Foreign Carriers</option>\n",
       "<option value=\"AS\">Alaska Airlines </option>\n",
       "<option value=\"G4\">Allegiant Air</option>\n",
       "<option value=\"AA\">American Airlines </option>\n",
       "<option value=\"5Y\">Atlas Air </option>\n",
       "<option value=\"DL\">Delta Air Lines </option>\n",
       "<option value=\"MQ\">Envoy Air</option>\n",
       "<option value=\"EV\">ExpressJet Airlines </option>\n",
       "<option value=\"F9\">Frontier Airlines </option>\n",
       "<option value=\"HA\">Hawaiian Airlines </option>\n",
       "<option value=\"B6\">JetBlue Airways</option>\n",
       "<option value=\"OO\">SkyWest Airlines </option>\n",
       "<option value=\"WN\">Southwest Airlines </option>\n",
       "<option value=\"NK\">Spirit Air Lines</option>\n",
       "<option value=\"UA\">United Air Lines </option>\n",
       "<option value=\"VX\">Virgin America</option>\n",
       "</select>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carrier_list = soup.find(id = \"CarrierList\")\n",
    "print(type(carrier_list))\n",
    "carrier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<option value=\"VX\">Virgin America</option>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = soup.find(value=\"VX\")\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extracting all the URLs found within a page’s  known tags:\n",
    "\n",
    "## element.get('tag') - to access the given elements attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links present in this webpage :  136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://www.transportation.gov',\n",
       " 'https://www.bts.gov/',\n",
       " 'http://transportation.libanswers.com/form.php?queue_id=1810',\n",
       " 'http://www.rita.dot.gov/bts/publications/',\n",
       " 'https://www.bts.dot.gov/explore-topics-and-geography',\n",
       " 'https://www.bts.dot.gov//topics/airlines-and-airports',\n",
       " 'https://www.bts.dot.gov//topics/energy-and-environment',\n",
       " 'https://www.bts.dot.gov//topics/freight-transportation',\n",
       " 'https://www.bts.dot.gov//topics/infrastructure',\n",
       " 'https://www.bts.dot.gov//topics/passenger-travel']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  extracting all the URLs found within all the '<a>' tags:\n",
    "page_links = []\n",
    "for link in soup.find_all('a'):\n",
    "    page_links.append(link.get('href'))\n",
    "    \n",
    "print(\"Total number of links present in this webpage : \",len(page_links))\n",
    "page_links[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting all the text from a page:\n",
    "\n",
    "## __soup.get_text() : __ returns all the text content in the given webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # The soup object contains all of the HTML in the original document\n",
    "\n",
    "__soup.preetify()__ : This method prints the HTML file in a more organized and comprehensive way.\n",
    "\n",
    "Beautiful Soup is essentially a set of wrapper functions that make it simple to select common HTML elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      " <head>\n",
      "  <title>\n",
      "   Data Elements\n",
      "  </title>\n",
      "  <link href=\"styles/global.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <link href=\"styles/rita_main.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <link href=\"https://fonts.googleapis.com/css?family=Open+Sans\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <link href=\"https://www.bts.dot.gov/sites/bts.dot.gov/themes/bts_standalone/bts_standalone.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"https://www.bts.dot.gov/sites/bts.dot.gov/themes/bts_standalone/bts_standalone_pn.css\" rel=\"stylesheet\"/>\n",
      "  <script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.2.6/jquery.min.js\" type=\"text/javascript\">\n",
      "  </script>\n",
      "  <script src=\"https://www.bts.dot.gov/sites/bts.dot.gov/themes/bts_standalone/bts_standalone.js\">\n",
      "  </script>\n",
      "  <script language=\"javascript\" type=\"text/javascript\">\n",
      "   function window_Carri\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify()[0:1000])  # Here we just took a slice of the soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all the carriers : \n",
      " ['All', 'AllUS', 'AllForeign', 'AS', 'G4', 'AA', '5Y', 'DL', 'MQ', 'EV', 'F9', 'HA', 'B6', 'OO', 'WN', 'NK', 'UA', 'VX']\n"
     ]
    }
   ],
   "source": [
    "carrier_list = []\n",
    "carrier_planes = soup.find(id = \"CarrierList\")   \n",
    "for carriers in carrier_planes.find_all('option'):\n",
    "    carrier_list.append(carriers['value'])\n",
    "\n",
    "print(\"List of all the carriers : \\n\",carrier_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_list = []\n",
    "airports = soup.find(id = 'AirportList')   # airports will have an html element as its value\n",
    "type(airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_list_name = []\n",
    "airports_name = soup.find(attrs = {'name': 'AirportList'})   \n",
    "type(airports_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of airports : \n",
      " 1209\n",
      "Some airports : \n",
      " ['WSM', 'OLF', 'ORH', 'WRL', 'WRG', 'YKM', 'YAK', 'XWC', 'WYB', 'YNG', 'A63', 'NYL', 'YUM', 'KZB', 'AK8']\n"
     ]
    }
   ],
   "source": [
    "for airport_names in airports.find_all('option'):\n",
    "    airport_list.append(airport_names['value'])\n",
    "    \n",
    "print(\"Total number of airports : \\n\",len(airport_list))\n",
    "print(\"Some airports : \\n\",airport_list[-15:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make a web scrapping it is important to understand how the site expects requests. So 1st step we have to findout which url we have to access and which http method to use. \n",
    "\n",
    "Here, __http method is post__ and for __url to access is : Data_elements.aspx?Data=2__ (i.e, to url Data_elements.aspx  we are passing the parameter Data=2). And that is exactly the url we have accessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"web2.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to mine, this site for request we need to learn how to programmatically construct request to pull each page of data we need. And each time we will be passing a carrier value and an airport value.\n",
    "\n",
    "So best way is to check how browser makes request to site.\n",
    "\n",
    "From the network tab of inspect we find that the httprequest has 8 parameters (including CarrierList and AirportList) in the __FORMDATA__. These 8 parameters are : \n",
    "* EVENTTARGET\n",
    "* EVENTARGUMENT\n",
    "* VIEWSTATE\n",
    "* CarrierList\n",
    "* AirportList\n",
    "* VIEWSTATEGENERATOR\n",
    "* EVENTVALIDATION\n",
    "* Submit\n",
    "\n",
    "These form elements which are needed to make the request are not part of the user interface. On checking they are hidden/present in the _div_ element. \n",
    "\n",
    "Now to make an httprequest so that this data is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")\n",
    "soup = BeautifulSoup(r.text,\"lxml\")\n",
    "div_hidden = soup.find(id = \"__EVENTVALIDATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eventvalidation = div_hidden['value']\n",
    "type(eventvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_hidden_view = soup.find(id = '__VIEWSTATE')\n",
    "# viewstate = div_hidden_view.find_all(['value'])  # wrong did only when we are using loop\n",
    "viewstate = div_hidden_view['value']\n",
    "type(viewstate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These form parameters   __VIEWSTATE__, __EVENTTARGET__ , __EVENTARGUMENT__ , __EVENTVALIDATION__ and __submit__, etc are used to validate each requests which are coming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",  # filling the value for Boston airport\n",
    "                          'CarrierList': \"VX\",   # filling the value for Virgin carrier\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# pprint(r.text)    # Returns the request object\n",
    "print(type(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433606"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"virgin_to_boston request2.html\",'w')\n",
    "f.write(r.text)\n",
    "# Unfortuantely, the HTML code on that page being scraped from has been \n",
    "# updated hence the scraping mechanism now fails. But conceptually what is being done here is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the data that we want we are getting syntax error. \n",
    "To solve these types of error some practices are : \n",
    "* Look at how a browser makes requests.\n",
    "* Emulate in code.\n",
    "* If stuff blows up, look at your http traffic.\n",
    "* Return to 1st step until it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this video was recorded, some changes to the code from line 13-20 are necessary in order to obtain the same functionality. First, the .post() function's first argument should be set to the secure server. Secondly, the \"data\" parameter should be a tuple of tuples in a specific order:\n",
    "\n",
    "Now, to check what the browser is doing differently there is a cookie above __FORMDATA__ with some session data so we can maintain a session state in the code. So we use a session object to get our both get and post. So session request will be maintained and past along when we maintain this request.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = requests.Session()\n",
    "r = s.get(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")\n",
    "soup = BeautifulSoup(r.text,\"lxml\")\n",
    "div_hidden = soup.find(id = \"__EVENTVALIDATION\")\n",
    "eventvalidation = div_hidden['value']\n",
    "div_hidden_view = soup.find(id = '__VIEWSTATE')\n",
    "viewstate = div_hidden_view['value']\n",
    "\n",
    "r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "           data = (\n",
    "                   (\"__EVENTTARGET\", \"\"),\n",
    "                   (\"__EVENTARGUMENT\", \"\"),\n",
    "                   (\"__VIEWSTATE\", viewstate),\n",
    "                   (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                   (\"CarrierList\", \"VX\"),\n",
    "                   (\"AirportList\", \"BOS\"),\n",
    "                   (\"Submit\", \"Submit\")\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344619"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"virgin_to_boston request2.html\",'w')\n",
    "f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So we get the right html page generated with the required data for virgin airlines and Logan airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
